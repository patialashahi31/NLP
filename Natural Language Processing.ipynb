{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "text = \"Hello, How are you? What are you doing?\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"This is some sample text,showing off the stop words filtration \"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example)\n",
    "sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(sentence)\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming words with NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['ride','riding','rider','rides']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming an entire sentence\n",
    "new_text = \"When riders are riding their horses , they often think of how cowboys rode horses.\"\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will tag each tokenized word with part of speech\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words  = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk corps/NN)\n",
      "(Chunk Today/VB)\n",
      "(Chunk nation/NN)\n",
      "(Chunk lost/VBD)\n",
      "(Chunk beloved/VBN)\n",
      "(Chunk woman/NN)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk founding/NN)\n",
      "(Chunk carried/VBD)\n",
      "(Chunk dream/NN)\n",
      "(Chunk Tonight/NN)\n",
      "(Chunk are/VBP comforted/VBN)\n",
      "(Chunk hope/NN)\n",
      "(Chunk reunion/NN)\n",
      "(Chunk husband/NN)\n",
      "(Chunk was/VBD taken/VBN)\n",
      "(Chunk so/RB long/RB ago/RB)\n",
      "(Chunk are/VBP)\n",
      "(Chunk life/NN)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk reacts/VBZ)\n",
      "(Chunk applause/VB)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b43615026d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprocess_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-b43615026d08>\u001b[0m in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mchunked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda2\\lib\\site-packages\\nltk\\tree.pyc\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mdraw_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda2\\lib\\site-packages\\nltk\\draw\\tree.pyc\u001b[0m in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m     \"\"\"\n\u001b[1;32m--> 863\u001b[1;33m     \u001b[0mTreeView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda2\\lib\\site-packages\\nltk\\draw\\tree.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m         \"\"\"\n\u001b[0;32m    853\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DELL\\Anaconda2\\lib\\lib-tk\\Tkinter.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Chunking\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "#Define a function that will tag each tokenized word with part of speech\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words  = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>*<NN>?}\"\"\"\n",
    "            chunkParser  = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                 print(subtree)\n",
    "            \n",
    "            \n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chinking\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "#Define a function that will tag each tokenized word with part of speech\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words  = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                           }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser  = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "               print(subtree)\n",
    "            \n",
    "            \n",
    "           # chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words  = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged,binary=False)\n",
    "            namedEnt.draw()\n",
    "            \n",
    "            \n",
    "           # chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text classification\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of doucments 2000\n",
      "first review ([u'scream', u'2', u'has', u'a', u'titillating', u'little', u'scene', u'that', u'lays', u'down', u'the', u'unwritten', u'law', u'of', u'horror', u'movie', u'sequels', u'quite', u'well', u'.', u'during', u'a', u'film', u'-', u'class', u'discussion', u',', u'windsor', u'college', u'students', u'articulate', u'what', u'these', u'inevitable', u'laws', u'are', u',', u'and', u'why', u'sequels', u'never', u'live', u'up', u'to', u'the', u'originals', u'.', u'a', u'few', u'rare', u'exceptions', u'are', u'noted', u',', u'but', u'they', u'left', u'out', u'one', u'.', u'.', u'.', u'scream', u'2', u'!', u'1996', u\"'\", u's', u'surprise', u'horror', u'blockbuster', u'scream', u'became', u'an', u'instant', u'hit', u'and', u'developed', u'an', u'obsessive', u'coterie', u'of', u'fans', u'which', u'included', u'almost', u'every', u'teenager', u'in', u'america', u'!', u'a', u'sequel', u'was', u'unavoidable', u',', u'but', u'luckily', u'the', u're', u'-', u'teaming', u'of', u'writer', u'kevin', u'williamson', u'and', u'director', u'wes', u'craven', u'gives', u'us', u'nothing', u'to', u'fear', u'in', u'the', u'way', u'of', u'lost', u'entertainment', u'.', u'in', u'fact', u',', u'at', u'the', u'risk', u'of', u'sounding', u'sacrilegious', u'to', u'all', u'the', u'adorning', u'fans', u'of', u'scream', u',', u'i', u'would', u'say', u'that', u'scream', u'2', u'might', u'even', u'be', u'the', u'better', u'film', u'!', u'sidney', u'prescott', u'(', u'neve', u'campbell', u')', u',', u'the', u'sinuous', u'heroine', u'who', u'survived', u'a', u'series', u'of', u'murders', u'in', u'woodsboro', u',', u'ca', u'in', u'the', u'original', u'scream', u',', u'is', u'now', u'two', u'years', u'older', u'and', u'off', u'to', u'college', u',', u'majoring', u'in', u'theater', u'.', u'meanwhile', u',', u'tv', u'journalist', u'gale', u'weathers', u'(', u'courtney', u'cox', u')', u',', u'who', u'covered', u'the', u'murders', u'in', u'the', u'first', u'film', u',', u'has', u'since', u'written', u'a', u'book', u'about', u'the', u'dreadful', u'events', u';', u'a', u'book', u'titled', u'\"', u'stab', u'\"', u',', u'which', u'has', u'been', u'developed', u'into', u'a', u'major', u'motion', u'picture', u'.', u'as', u'scream', u'2', u'opens', u',', u'a', u'crowd', u'of', u'overzealous', u'teenagers', u'are', u'piling', u'into', u'a', u'crowded', u'movie', u'theater', u'for', u'the', u'premiere', u'of', u'\"', u'stab', u'\"', u',', u'and', u'before', u'long', u',', u'a', u'young', u'couple', u'are', u'butchered', u'to', u'death', u',', u'unnoticed', u'in', u'the', u'midst', u'of', u'hysteria', u'.', u'this', u'marks', u'the', u'beginning', u'of', u'a', u'new', u'murder', u'spree', u'as', u'an', u'unknown', u'\"', u'copycat', u'\"', u'sets', u'out', u'to', u'repeat', u'what', u\"'\", u's', u'been', u'done', u'.', u'.', u'.', u'and', u'finish', u'what', u'hasn', u\"'\", u't', u'!', u'of', u'course', u'the', u'killer', u'is', u'just', u'as', u'obsessed', u'with', u'movies', u'as', u'the', u'killer', u'in', u'the', u'first', u'film', u',', u'and', u'yes', u',', u'movies', u'play', u'a', u'huge', u'role', u'in', u'the', u'bizarre', u'psychological', u'outlook', u'the', u'murderer', u'holds', u'as', u'he', u'/', u'she', u'goes', u'about', u'slaying', u'an', u'assortment', u'of', u'gorgeous', u'twenty', u'year', u'olds', u',', u'but', u'this', u'*', u'is', u'*', u'a', u'sequel', u',', u'and', u'without', u'deviating', u'from', u'the', u'original', u'groundwork', u'we', u'are', u'still', u'given', u'a', u'fresh', u'outlook', u'on', u'horror', u'movies', u'.', u'this', u'is', u'in', u'large', u'part', u'due', u'to', u'williamson', u\"'\", u's', u'deliciously', u'written', u'script', u',', u'which', u'will', u'easily', u'leave', u'you', u'satisfied', u'.', u'as', u'much', u'as', u'the', u'film', u'condemns', u'the', u'use', u'of', u'cliches', u',', u'it', u'is', u'riddled', u'with', u'them', u',', u'but', u'it', u'is', u'done', u'so', u'effectively', u',', u'and', u'with', u'skillful', u'direction', u'by', u'craven', u',', u'that', u'you', u\"'\", u're', u'not', u'bound', u'to', u'complain', u'.', u'i', u'will', u'note', u',', u'however', u',', u'that', u'the', u'\"', u'whodunit', u'?', u'\"', u'did', u'wind', u'up', u'being', u'the', u'first', u'person', u'i', u'suspected', u',', u'but', u'believe', u'me', u',', u'i', u'kept', u'changing', u'my', u'mind', u'as', u'i', u'tried', u'to', u'stay', u'one', u'ahead', u'of', u'the', u'movie', u'.', u'all', u'the', u'actors', u'outdo', u'themselves', u'here', u'.', u'we', u\"'\", u're', u'not', u'talking', u'oscar', u'performances', u',', u'but', u'cambell', u',', u'cox', u',', u'david', u'arquette', u'as', u'dewey', u'riley', u',', u'and', u'jamie', u'kennedy', u'as', u'randy', u'all', u'give', u'surprisingly', u'satisfying', u'performances', u'that', u'go', u'beyond', u'their', u'last', u'outing', u'.', u'the', u'characters', u'remain', u'true', u'to', u'their', u'roots', u',', u'and', u'yet', u'show', u'a', u'clear', u'sign', u'of', u'development', u'over', u'the', u'last', u'two', u'years', u'.', u'we', u'can', u'see', u'how', u'the', u'events', u'in', u'the', u'first', u'film', u'has', u'affected', u'them', u',', u'but', u'we', u'don', u\"'\", u't', u'lose', u'any', u'of', u'the', u'luster', u'that', u'made', u'them', u'such', u'a', u'delight', u'to', u'watch', u'in', u'the', u'first', u'place', u'.', u'the', u'nice', u'thing', u'about', u'scream', u'2', u'is', u'that', u'it', u'doesn', u\"'\", u't', u'seem', u'off', u'-', u'kilter', u'from', u'it', u\"'\", u's', u'predecessor', u'.', u'it', u\"'\", u's', u'a', u'natural', u',', u'smooth', u',', u'and', u'believable', u'(', u'as', u'far', u'as', u'horror', u'films', u'go', u')', u'transition', u'from', u'film', u'to', u'film', u'.', u'we', u'skip', u'two', u'years', u',', u'but', u'there', u'doesn', u\"'\", u't', u'seem', u'to', u'be', u'any', u'holes', u'or', u'shortcuts', u'taken', u'in', u'connecting', u'the', u'two', u'flicks', u'.', u'it', u'flows', u'so', u'well', u'that', u'you', u'feel', u'more', u'like', u'you', u\"'\", u're', u'reading', u'chapter', u'two', u'of', u'a', u'book', u'(', u'long', u'chapters', u',', u'eh', u'?', u')', u'than', u'just', u'revitalizing', u'a', u'cast', u'of', u'familiar', u'faces', u'.', u'while', u'main', u'characters', u'are', u'usually', u'the', u'only', u'connection', u'in', u'sequels', u',', u'scream', u'2', u\"'\", u's', u'entire', u'plot', u'structure', u'correlates', u'*', u'completely', u'*', u'with', u'the', u'first', u',', u'making', u'for', u'immediate', u'fondness', u'and', u'absorption', u'.', u'as', u'i', u'said', u'before', u',', u'scream', u'2', u'easily', u'lives', u'up', u'to', u',', u'if', u'not', u'surpassing', u',', u'it', u\"'\", u's', u'precursor', u'.', u'the', u'only', u'reason', u'scream', u'will', u'likely', u'be', u'hailed', u'above', u'scream', u'2', u'is', u'because', u'it', u'was', u'the', u'first', u'.', u'this', u'doesn', u\"'\", u't', u'mean', u'that', u'it', u'overcasts', u'the', u'unique', u',', u'whimsical', u'humor', u'or', u'overall', u'caliber', u'of', u'it', u\"'\", u's', u'sequel', u',', u'it', u'just', u'means', u'that', u'scream', u'will', u'forever', u'be', u'known', u'as', u'the', u'movie', u'that', u'mocked', u'it', u\"'\", u's', u'own', u'while', u'maintaining', u'it', u\"'\", u's', u'sought', u'-', u'after', u'qualities', u'.', u'if', u'ever', u'a', u'film', u'followed', u'it', u\"'\", u's', u'original', u'so', u'well', u',', u'it', u'would', u'be', u'scream', u'2', u',', u'and', u'it', u'makes', u'this', u'a', u'definite', u'must', u'-', u'see', u'for', u'all', u'those', u'unbridled', u'scream', u'fans', u'out', u'there', u'.', u'scream', u'2', u'fits', u'snugly', u'alongside', u'scream', u',', u'and', u'will', u'hopefully', u'receive', u'as', u'many', u'high', u'remarks', u'.', u'it', u'would', u'top', u'off', u'the', u'series', u'quite', u'well', u',', u'if', u'it', u'weren', u\"'\", u't', u'for', u'the', u'fact', u'that', u'two', u'movies', u'don', u\"'\", u't', u'really', u'make', u'a', u'series', u'.', u'it', u\"'\", u's', u'unlikely', u'to', u'assume', u'that', u'a', u'scream', u'3', u'won', u\"'\", u't', u'appear', u'in', u'the', u'next', u'few', u'years', u',', u'but', u'if', u'they', u'do', u'decide', u'to', u'turn', u'this', u'into', u'an', u'undeniably', u'acclaimed', u'horror', u'-', u'trilogy', u',', u'let', u\"'\", u's', u'hope', u'they', u'can', u'keep', u'up', u'the', u'fantastic', u'work', u'.', u'like', u'they', u'say', u',', u'don', u\"'\", u't', u'mess', u'with', u'perfection', u'.', u'.', u'.', u'but', u'when', u'a', u'money', u'-', u'making', u'smash', u'-', u'hit', u'is', u'almost', u'guaranteed', u',', u'who', u\"'\", u's', u'gonna', u'listen', u'?'], u'pos')\n",
      "Most common words [(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n",
      "the word happy 215\n"
     ]
    }
   ],
   "source": [
    "#build  a list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)),category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)] \n",
    "\n",
    "#shuffle documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(\"No. of doucments {}\".format(len(documents)))\n",
    "print(\"first review {}\".format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print(\"Most common words {}\".format(all_words.most_common(15)))\n",
    "print(\"the word happy {}\".format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use the 4000 most common words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shows\n",
      "kids\n",
      "plot\n",
      "music\n",
      "want\n",
      "production\n",
      "feeling\n",
      "away\n",
      ".\n",
      "has\n",
      "confusing\n",
      "bottom\n",
      "exact\n",
      "years\n",
      "still\n",
      "now\n",
      "didn\n",
      "one\n",
      "s\n",
      "world\n",
      "arrow\n",
      "with\n",
      "concept\n",
      "7\n",
      "horror\n",
      "more\n",
      "visions\n",
      "american\n",
      "feels\n",
      "also\n",
      "into\n",
      "video\n",
      "makes\n",
      "start\n",
      "tons\n",
      "despite\n",
      "meantime\n",
      "'\n",
      "insight\n",
      "off\n",
      "not\n",
      "wes\n",
      "problem\n"
     ]
    }
   ],
   "source": [
    "#Build a find_feature function that will determine which of 4000 word features are contained in review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key,value in features.items():\n",
    "    if value ==True:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do it for all the documents\n",
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets \n",
    "from sklearn import model_selection\n",
    "seed =1\n",
    "training,testing = model_selection.train_test_split(featuresets,test_size=0.25,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how we use sklearn in nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuray 0.656\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "accuracy = nltk.classify.accuracy(model,testing)\n",
    "print(\"SVC accuray {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
